{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Neural Network with Keras\n",
    "In this tutorial, you will build your first **multilayer perceptron (MLP)** model (also your first neural network) in Python with Keras. As you will see, Keras is a easy-to-use deep API that allows you to easily build, train, evaluate and execute deep learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n",
    "In this tutorial, we are going to use Pima Indians Diabetes dataset which is a standard machine learning dataset from the UCI Machine Learning repository. The dataset can be downloaded from <a href=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\" target=\"_blank\">here</a>. Place it in the same location as this notebook file.\n",
    "\n",
    "This is a comparably small dataset, with 768 samples in total. There are eight input features (X) and one output variable (y).\n",
    "\n",
    "Input features (X):\n",
    "\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "\n",
    "Output Variables (y):\n",
    "\n",
    "1. Class label (1 for diabetes, 0 for not)\n",
    "\n",
    "First we need import all necessory libraries which we will need to work upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-14T14:55:47.833267Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `numpy` function `loadtxt` to load the CSV file. In the file, each row corresponds to one example, with nine columns. We can then split them into the input features (X) and output labels (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loadtxt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mloadtxt\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpima-indians-diabetes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# split into input X and output y variables\u001b[39;00m\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m8\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loadtxt' is not defined"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv',delimiter=',')\n",
    "# split into input X and output y variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the dimension of X. You may also want to have a look of a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dataset[i])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "for i in range(5):\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split them into training set and test set by using the `train_test_split( )` method from `sklearn`. `test_size = 0.2` means 20% examples are used for test. \n",
    "\n",
    "You must **treat the test set as unseen data**, which means any model adjustment must be done only on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the data\n",
    "Usually there is a pre-processing step before building the model. What pre-processing should be carried out depends on the data and any requirement for the model. For this dataset, the eight features are in variant ranges and scales, which may lead to unstable weight learning. In this case, the common practice is to standardize the input data, to make each feature to be mean 0 and unit variance. The network would then be trained on a more stable distribution of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train = X_train.mean(axis=0)\n",
    "std_train = X_train.std(axis=0)\n",
    "X_train = (X_train-mean_train)/std_train\n",
    "X_test = (X_test-mean_train)/std_train # Apply the same transformation on the testing set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build your network\n",
    "Now we are ready to build our neural network. In Keras, we first create a `sequential` model, then add layers one by one. \n",
    "\n",
    "As there are only 8 input features in this dataset, we may consider to build a small-size MLP: three fully connected layers (or `Dense` layers). For each `Dense` layer, we specify the number of neurons in the layer as the first argument, and specify the activation function using the `activation` argument. For the first `Dense` layer, we also need give the input feature dimension (`input_dim=8`).\n",
    "\n",
    "We use the Rectified Linear Unit (ReLU) activation function for the 2 hidden layers, as ReLU usually produces better performance compared to the Sigmoid or Tanh functions. We use a sigmoid on the output layer to ensure the network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n",
    "\n",
    "Now lets's interprete the following code as follows.\n",
    "\n",
    "1. The model expects the input data containing 8 features (or columns).\n",
    "2. We add the first hidden layer with 12 neurons and use the ReLU activation fuction.\n",
    "3. We add the second hidden layer with 6 neurons and use the ReLu activation function.\n",
    "4. Finally, we add the output layer with 1 neuron and use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a keras model of a MLP network with three Dense layers\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation = 'relu'))\n",
    "model.add(Dense(6,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `summary()` method to display all the model's layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layers often have a lot of parameters. Even we build such a small MLP, there are 193 parameters in total. Let's see how the numbers are calculated. For example, the first hidden layer has 8 x 12 connection weights, plus 8 bias terms, which adds up to 108 parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compile the model\n",
    "After the model is created, you must call the `compile()` method to specify the `loss` function and the `optimizer` to use. Optionally, you can specify a list of extra `metrics` to compute and report during training and evaluation.\n",
    "\n",
    "Regarding the binary classification problem, the typical loss function is `binary_crossentropy` defined in Keras. \n",
    "\n",
    "We choose the efficient stochastic gradient descent algorithm `adam` as the optimizer. It is a popular choice as it adaptively chooses the learning rate and gives good results in a wide range of problems. \n",
    "\n",
    "Since it is a classification problem, it is more intuitive to measure and report the classification `accuracy`, defined via the `metrics` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and validate the model\n",
    "Now the model is ready to be trained. For this we simply call its `fit()` method. We'll train the model for 120 epochs (120 itrations over all samples in the training set), in mini-batches of 32 samples. (If you don't know what mini-batch means, don't worry, we will talk about it in our later lectures.)\n",
    "\n",
    "We also split 25% examples in the training set as a validation set (this is optional). Keras will measure the loss and the accuracy on this validation set at the end of each epoch, which is very useful to see how well the model performs. Based on the performance on the validation set, we could modify the model and tune the hyperparameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.6377 - accuracy: 0.7000 - val_loss: 0.6184 - val_accuracy: 0.7143\n",
      "Epoch 2/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6217 - accuracy: 0.7130 - val_loss: 0.6038 - val_accuracy: 0.7273\n",
      "Epoch 3/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.7043 - val_loss: 0.5902 - val_accuracy: 0.7273\n",
      "Epoch 4/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.6978 - val_loss: 0.5777 - val_accuracy: 0.7338\n",
      "Epoch 5/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5865 - accuracy: 0.7087 - val_loss: 0.5673 - val_accuracy: 0.7208\n",
      "Epoch 6/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5762 - accuracy: 0.7196 - val_loss: 0.5583 - val_accuracy: 0.7208\n",
      "Epoch 7/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7391 - val_loss: 0.5491 - val_accuracy: 0.7143\n",
      "Epoch 8/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5559 - accuracy: 0.7478 - val_loss: 0.5407 - val_accuracy: 0.7143\n",
      "Epoch 9/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5455 - accuracy: 0.7609 - val_loss: 0.5317 - val_accuracy: 0.7143\n",
      "Epoch 10/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7674 - val_loss: 0.5245 - val_accuracy: 0.7013\n",
      "Epoch 11/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7674 - val_loss: 0.5192 - val_accuracy: 0.7143\n",
      "Epoch 12/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7761 - val_loss: 0.5137 - val_accuracy: 0.7143\n",
      "Epoch 13/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7761 - val_loss: 0.5101 - val_accuracy: 0.7143\n",
      "Epoch 14/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7804 - val_loss: 0.5071 - val_accuracy: 0.7208\n",
      "Epoch 15/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.7848 - val_loss: 0.5042 - val_accuracy: 0.7208\n",
      "Epoch 16/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4939 - accuracy: 0.7891 - val_loss: 0.5023 - val_accuracy: 0.7208\n",
      "Epoch 17/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7891 - val_loss: 0.5006 - val_accuracy: 0.7208\n",
      "Epoch 18/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7913 - val_loss: 0.4996 - val_accuracy: 0.7208\n",
      "Epoch 19/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7891 - val_loss: 0.4986 - val_accuracy: 0.7273\n",
      "Epoch 20/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7826 - val_loss: 0.4974 - val_accuracy: 0.7273\n",
      "Epoch 21/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7848 - val_loss: 0.4976 - val_accuracy: 0.7273\n",
      "Epoch 22/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.7826 - val_loss: 0.4962 - val_accuracy: 0.7273\n",
      "Epoch 23/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.7870 - val_loss: 0.4961 - val_accuracy: 0.7273\n",
      "Epoch 24/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.7870 - val_loss: 0.4954 - val_accuracy: 0.7273\n",
      "Epoch 25/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7913 - val_loss: 0.4951 - val_accuracy: 0.7273\n",
      "Epoch 26/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7913 - val_loss: 0.4960 - val_accuracy: 0.7273\n",
      "Epoch 27/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7848 - val_loss: 0.4964 - val_accuracy: 0.7273\n",
      "Epoch 28/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4579 - accuracy: 0.7848 - val_loss: 0.4970 - val_accuracy: 0.7273\n",
      "Epoch 29/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7804 - val_loss: 0.4968 - val_accuracy: 0.7208\n",
      "Epoch 30/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.7826 - val_loss: 0.4979 - val_accuracy: 0.7273\n",
      "Epoch 31/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4534 - accuracy: 0.7870 - val_loss: 0.4978 - val_accuracy: 0.7208\n",
      "Epoch 32/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.7870 - val_loss: 0.4990 - val_accuracy: 0.7208\n",
      "Epoch 33/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.7848 - val_loss: 0.4990 - val_accuracy: 0.7208\n",
      "Epoch 34/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.7870 - val_loss: 0.4993 - val_accuracy: 0.7208\n",
      "Epoch 35/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4486 - accuracy: 0.7848 - val_loss: 0.5004 - val_accuracy: 0.7208\n",
      "Epoch 36/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4473 - accuracy: 0.7848 - val_loss: 0.5007 - val_accuracy: 0.7208\n",
      "Epoch 37/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.7826 - val_loss: 0.5018 - val_accuracy: 0.7208\n",
      "Epoch 38/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.7826 - val_loss: 0.5021 - val_accuracy: 0.7208\n",
      "Epoch 39/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7848 - val_loss: 0.5040 - val_accuracy: 0.7208\n",
      "Epoch 40/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7826 - val_loss: 0.5051 - val_accuracy: 0.7208\n",
      "Epoch 41/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.7826 - val_loss: 0.5066 - val_accuracy: 0.7208\n",
      "Epoch 42/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.7826 - val_loss: 0.5065 - val_accuracy: 0.7273\n",
      "Epoch 43/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7848 - val_loss: 0.5073 - val_accuracy: 0.7208\n",
      "Epoch 44/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7826 - val_loss: 0.5089 - val_accuracy: 0.7208\n",
      "Epoch 45/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.7848 - val_loss: 0.5087 - val_accuracy: 0.7208\n",
      "Epoch 46/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.7870 - val_loss: 0.5088 - val_accuracy: 0.7208\n",
      "Epoch 47/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4371 - accuracy: 0.7870 - val_loss: 0.5102 - val_accuracy: 0.7208\n",
      "Epoch 48/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.7870 - val_loss: 0.5136 - val_accuracy: 0.7338\n",
      "Epoch 49/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4356 - accuracy: 0.7891 - val_loss: 0.5145 - val_accuracy: 0.7338\n",
      "Epoch 50/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.7913 - val_loss: 0.5142 - val_accuracy: 0.7403\n",
      "Epoch 51/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.7935 - val_loss: 0.5125 - val_accuracy: 0.7338\n",
      "Epoch 52/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.7935 - val_loss: 0.5151 - val_accuracy: 0.7338\n",
      "Epoch 53/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.7935 - val_loss: 0.5162 - val_accuracy: 0.7338\n",
      "Epoch 54/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.7935 - val_loss: 0.5179 - val_accuracy: 0.7338\n",
      "Epoch 55/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4311 - accuracy: 0.7913 - val_loss: 0.5193 - val_accuracy: 0.7468\n",
      "Epoch 56/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.7891 - val_loss: 0.5195 - val_accuracy: 0.7403\n",
      "Epoch 57/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.7935 - val_loss: 0.5210 - val_accuracy: 0.7338\n",
      "Epoch 58/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.7913 - val_loss: 0.5203 - val_accuracy: 0.7403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.7913 - val_loss: 0.5224 - val_accuracy: 0.7468\n",
      "Epoch 60/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.7848 - val_loss: 0.5229 - val_accuracy: 0.7468\n",
      "Epoch 61/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.7891 - val_loss: 0.5227 - val_accuracy: 0.7468\n",
      "Epoch 62/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4265 - accuracy: 0.7913 - val_loss: 0.5233 - val_accuracy: 0.7468\n",
      "Epoch 63/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.7935 - val_loss: 0.5252 - val_accuracy: 0.7403\n",
      "Epoch 64/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.7935 - val_loss: 0.5250 - val_accuracy: 0.7468\n",
      "Epoch 65/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.7913 - val_loss: 0.5267 - val_accuracy: 0.7468\n",
      "Epoch 66/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.7935 - val_loss: 0.5270 - val_accuracy: 0.7468\n",
      "Epoch 67/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.7935 - val_loss: 0.5287 - val_accuracy: 0.7468\n",
      "Epoch 68/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.7913 - val_loss: 0.5285 - val_accuracy: 0.7403\n",
      "Epoch 69/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.7848 - val_loss: 0.5310 - val_accuracy: 0.7403\n",
      "Epoch 70/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.7848 - val_loss: 0.5292 - val_accuracy: 0.7468\n",
      "Epoch 71/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4209 - accuracy: 0.7935 - val_loss: 0.5286 - val_accuracy: 0.7468\n",
      "Epoch 72/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.7891 - val_loss: 0.5314 - val_accuracy: 0.7468\n",
      "Epoch 73/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.7913 - val_loss: 0.5323 - val_accuracy: 0.7468\n",
      "Epoch 74/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4187 - accuracy: 0.7870 - val_loss: 0.5333 - val_accuracy: 0.7468\n",
      "Epoch 75/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.7870 - val_loss: 0.5340 - val_accuracy: 0.7468\n",
      "Epoch 76/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4169 - accuracy: 0.7870 - val_loss: 0.5337 - val_accuracy: 0.7403\n",
      "Epoch 77/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4160 - accuracy: 0.7913 - val_loss: 0.5351 - val_accuracy: 0.7468\n",
      "Epoch 78/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.7935 - val_loss: 0.5359 - val_accuracy: 0.7403\n",
      "Epoch 79/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.7913 - val_loss: 0.5374 - val_accuracy: 0.7403\n",
      "Epoch 80/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.7957 - val_loss: 0.5354 - val_accuracy: 0.7468\n",
      "Epoch 81/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4137 - accuracy: 0.7957 - val_loss: 0.5352 - val_accuracy: 0.7468\n",
      "Epoch 82/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4132 - accuracy: 0.7957 - val_loss: 0.5337 - val_accuracy: 0.7468\n",
      "Epoch 83/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.7957 - val_loss: 0.5362 - val_accuracy: 0.7468\n",
      "Epoch 84/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.7935 - val_loss: 0.5365 - val_accuracy: 0.7468\n",
      "Epoch 85/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.7957 - val_loss: 0.5373 - val_accuracy: 0.7468\n",
      "Epoch 86/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.7957 - val_loss: 0.5398 - val_accuracy: 0.7468\n",
      "Epoch 87/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.7978 - val_loss: 0.5378 - val_accuracy: 0.7468\n",
      "Epoch 88/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4089 - accuracy: 0.7978 - val_loss: 0.5388 - val_accuracy: 0.7468\n",
      "Epoch 89/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4084 - accuracy: 0.7957 - val_loss: 0.5405 - val_accuracy: 0.7468\n",
      "Epoch 90/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4084 - accuracy: 0.7891 - val_loss: 0.5420 - val_accuracy: 0.7403\n",
      "Epoch 91/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4070 - accuracy: 0.7978 - val_loss: 0.5373 - val_accuracy: 0.7468\n",
      "Epoch 92/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8000 - val_loss: 0.5390 - val_accuracy: 0.7468\n",
      "Epoch 93/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.7978 - val_loss: 0.5400 - val_accuracy: 0.7468\n",
      "Epoch 94/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8022 - val_loss: 0.5412 - val_accuracy: 0.7468\n",
      "Epoch 95/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8000 - val_loss: 0.5434 - val_accuracy: 0.7532\n",
      "Epoch 96/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.7935 - val_loss: 0.5444 - val_accuracy: 0.7468\n",
      "Epoch 97/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.7913 - val_loss: 0.5451 - val_accuracy: 0.7468\n",
      "Epoch 98/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.7978 - val_loss: 0.5444 - val_accuracy: 0.7532\n",
      "Epoch 99/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.8000 - val_loss: 0.5420 - val_accuracy: 0.7597\n",
      "Epoch 100/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8043 - val_loss: 0.5438 - val_accuracy: 0.7532\n",
      "Epoch 101/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8022 - val_loss: 0.5469 - val_accuracy: 0.7532\n",
      "Epoch 102/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8000 - val_loss: 0.5476 - val_accuracy: 0.7532\n",
      "Epoch 103/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4000 - accuracy: 0.8022 - val_loss: 0.5484 - val_accuracy: 0.7532\n",
      "Epoch 104/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8000 - val_loss: 0.5488 - val_accuracy: 0.7532\n",
      "Epoch 105/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8065 - val_loss: 0.5485 - val_accuracy: 0.7532\n",
      "Epoch 106/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8065 - val_loss: 0.5484 - val_accuracy: 0.7532\n",
      "Epoch 107/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.8065 - val_loss: 0.5501 - val_accuracy: 0.7532\n",
      "Epoch 108/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8065 - val_loss: 0.5508 - val_accuracy: 0.7532\n",
      "Epoch 109/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8109 - val_loss: 0.5533 - val_accuracy: 0.7532\n",
      "Epoch 110/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 0.8043 - val_loss: 0.5524 - val_accuracy: 0.7532\n",
      "Epoch 111/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8043 - val_loss: 0.5533 - val_accuracy: 0.7532\n",
      "Epoch 112/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8022 - val_loss: 0.5556 - val_accuracy: 0.7532\n",
      "Epoch 113/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8043 - val_loss: 0.5552 - val_accuracy: 0.7597\n",
      "Epoch 114/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8022 - val_loss: 0.5582 - val_accuracy: 0.7532\n",
      "Epoch 115/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8043 - val_loss: 0.5585 - val_accuracy: 0.7532\n",
      "Epoch 116/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8043 - val_loss: 0.5583 - val_accuracy: 0.7597\n",
      "Epoch 117/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8065 - val_loss: 0.5598 - val_accuracy: 0.7532\n",
      "Epoch 118/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8065 - val_loss: 0.5617 - val_accuracy: 0.7597\n",
      "Epoch 119/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8043 - val_loss: 0.5603 - val_accuracy: 0.7597\n",
      "Epoch 120/120\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3915 - accuracy: 0.8000 - val_loss: 0.5581 - val_accuracy: 0.7597\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(X_train,y_train, epochs=120, batch_size=32, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the call to` model.fit()` returns a `history` object. This `history` object contains a member `history`, which is a dictionary containing the loss and extra metrics it monitored during the training process. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use this dictionary to create a `panda` `DataFrame` and call its `plot()` method, you will get the four learning curves plotted in one figure. We will plot them in two separate figures, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEKCAYAAAACS67iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUVfr48c/JTHqvlIQQQIpACr03UZq9sGBDUPTr2lb9LYu6rrqW1bXsuroqomJfEUVWVlFX2ASwICC9hQ4pENJ7nZzfH3cISUgDZjLh5nm/XvPKzL1nzjl38uTJmTP3nlFaa4QQQpz/3FzdASGEEI4hCV0IIUxCEroQQpiEJHQhhDAJSehCCGESktCFEMIkmk3oSqlFSqkTSqkdjexXSqlXlFL7lVLblFIDHd9NIRxPYluYTUtG6O8BU5rYPxXoab/dAbxx7t0SolW8h8S2MJFmE7rWeg2Q00SRK4EPtGEdEKSU6uSoDgrhLBLbwmysDqgjEkip9TjVvu1Y/YJKqTswRjp4eXkNio6OdkDzZ6e6uho3N9d9hODK9tvDse/duzdLax1+jtVIbJ9HbbeX9puMba11szcgBtjRyL6vgdG1Hq8CBjVXZ69evbQrJSYmttv228OxAxu1xHa7aru9tN9UbDviX0kq0KXW4ygg3QH1CuFqEtvivOKIhL4cmGU/I2A4kK+1Pu0tqRDnIYltcV5pdg5dKfUJMB4IU0qlAo8D7gBa6wXACmAasB8oAeY4q7NCOJLEtjCbZhO61vr6ZvZr4G6H9aidqaysJDU1lbKyslZrMzAwkN27d7dae85s38vLi6ioKNzd3c/4ue0xtp0db2aKLVe3fzax7YizXMQ5SE1Nxd/fn5iYGJRSrdJmYWEh/v7+rdKWM9vXWpOdnU1qairdunVzQM/Mz9nxZpbYcnX7Zxvbcum/i5WVlREaGtpqydxMlFKEhoa26rub853E2/nhbGNbEnobIH9cZ09euzMnr9n54Wx+T5LQhRDCJCSht3N5eXm8/vrrZ/XcadOmkZeX1+LyTzzxBC+++OJZtSXMoTXjrT2ShN7ONfUHZrPZmnzuihUrCAoKcka3hEmZMd601lRXV7u6G4Ak9HbvoYce4sCBAyQkJDBv3jySkpKYMGECN9xwA7GxsQBcddVVDBo0iH79+rFw4cKa58bExJCVlcXhw4e58MILuf322+nXrx+TJk2itLS0yXa3bNnC8OHDiYuL4+qrryY3NxeAV155hb59+xIXF8fMmTMBWL16NQkJCSQkJDBgwAAKCwud9GoIZ2vNePvPf/7DsGHDGDBgABdffDEZGRkAFBUVMWfOHGJjY4mLi2Pp0qUAfPvttwwcOJD4+HgmTpwInP6usn///hw+fLimD3fddRcDBw4kJSWF3/72t4wbN45+/frx+OOP1zxnw4YNjBw5kvj4eIYOHUphYSFjxoxhy5YtNWVGjRrFtm3bzvn1ldMW25A//2cnu9ILHFpn384BPH55v0b3P/fcc+zYsaMmuJKSkli/fj07duyoOV1q0aJFhISEUFpaypAhQ7j22msJDQ2tU8++ffv45JNPeOutt/jNb37D0qVLuemmmxptd9asWbz66quMGzeOxx57jD//+c+8/PLLPPfccxw6dAhPT8+at9cvvvgir732GqNGjaKoqAgvL69zfVkEzom3nmHePH1tQqP7WzPeRo8ezbp161BK8fbbb/P888/z0ksv8dRTTxEYGMj27dsByM3NJTMzk9tvv501a9bQrVs3cnKaWoTTkJyczLvvvlvzjuOZZ57B3d0dHx8fJk6cyLZt2+jTpw8zZszg008/ZciQIRQUFODt7c3cuXN57733ePnll9m7dy/l5eXExcW1/IVuhIzQxWmGDh1a59zXV155hfj4eIYPH05KSgr79u077TndunUjIcH4Qx40aBCHDx9utP78/Hzy8vIYN24cALfccgtr1qwBIC4ujhtvvJGPPvoIq9UYb4waNYoHH3yQV155hby8vJrtwhycFW+pqalMnjyZ2NhYXnjhBXbu3AnAypUrufvuU9eLBQcHs27dOsaOHVvTj5CQkGb73bVrV4YPH17zeMmSJYwZM4YBAwawc+dOdu3aRXJyMp06dWLIkCEABAQEYLVamT59Ol999RWVlZUsWrSI2bNnN/9CtYD8ZbQhTY2kW5Ovr2/N/aSkJFauXMnPP/+Mj48P48ePb/DcWE9Pz5r7Foul2SmXxnz99desWbOG5cuX89RTT7Fz504eeughLr30UlasWMHw4cNZuXIlffr0Oav6xSnOiLezmQ5zVrzde++9PPjgg1xxxRUkJSXxxBNPAMacd/1TAhvaBmC1WuvMj9fuS+1+Hzp0iBdffJH//e9/REdHM3v2bMrKyhqt18fHh0suuYQvv/ySJUuWsHHjxoZemjMmI/R2zt/fv8k/wvz8fIKDg/Hx8WHPnj2sW7funNsMDAwkODiYtWvXAvDhhx8ybtw4qqurSUlJYcKECTz//PPk5eVRVFTEgQMHiI2NZf78+QwePJg9e/accx+Ea7RmvOXn5xMZGQnA+++/X7N90qRJ/POf/6x5nJuby4gRI1i9ejWHDh0CqJlyiYmJYdOmTQBs2rSpZn99BQUF+Pr6EhgYSEZGBt988w0Affr0IT09nQ0bNgDGP7yqqioA5s6dy3333ceQIUNa9I6gJSSht3OhoaGMGjWK/v37M2/evNP2T5kyhaqqKuLi4vjTn/5U5y3muXj//feZN28ecXFxbNmyhcceewybzcZNN91EbGwsAwYM4IEHHiAoKIiXX36Z/v37Ex8fj7e3N1OnTnVIH0Tra814e+KJJ5g+fTpjxowhLCysZvujjz5Kbm5uTUwlJiYSHh7OwoULueaaa4iPj2fGjBkAXHvtteTk5JCQkMAbb7xBr169GmwrPj6eAQMGMHToUG699VZGjRoFgIeHB59++in33nsv8fHxXHLJJTWj/EGDBhEQEMCcOQ5c862xhdKdfWvPXwJQu/1du3a1etsFBQWt3qYz22/oNaSFX3DhjFtbjm1nx5vZYsuZ7aelpemePXtqm83WaJkzjW0ZoQshRCv74IMPGDZsGM8884xDv7JOPhQVQohWNmvWLGbNmuXwemWELoQQJiEJXQghTEISuhBCmIQkdCGEMAlJ6OKM+fn5ndF2Ic6FxFXLSUIXQogmnLyy83wgCb2dmz9/fp31qZ944gleeuklioqKmDhxIgMHDiQ2NpYvv/yyxXVqrZk3bx79+/cnNjaWTz/9FIBjx44xduzYmisF165di81mY/bs2TVl//73vzv8GEXb4ch4a2yZ3YaWwW1sydzao//PP/+8ZpGs2bNn8+CDDzJhwgTmz5/P+vXrGTlyJAMGDGDkyJEkJycDxhruv//972vqXbBgAatWreLqq6+uqff777/nmmuuOfsX7QzIeehtyTcPwfHtjq2zYyxMfa7R3TNnzuT+++/nrrvuAowV47799lu8vLxYtmwZAQEBZGVlMXz4cK644ooWfc/hF198wZYtW9i6dStZWVkMGTKEsWPH8q9//YvJkydz33334ePjQ0lJCVu2bCEtLY0dO3YAyDfStCYnxJtnaG+44m+N7ndkvDW0zG5BQUGDy+A2tGRuc/bu3cvKlSuxWCwUFBSwZs0arFYrK1eu5JFHHmHp0qUsXLiQQ4cOsXnzZqxWK0eOHCE6Opq7776bzMxMwsPDeffddx17eX8TJKG3cwMGDODEiROkp6eTmZlJcHAw0dHRVFZW8sgjj7BmzRrc3NxIS0sjIyODjh07NlvnDz/8wPXXX4/FYqFDhw6MGzeODRs2MGTIEG699VaKioqYMWMGCQkJdO/enYMHD3Lvvfdy6aWXMmnSpFY4auEqjoy3V155hWXLlgHULLN79OjRBpfBXblyJYsXL655bnBwcLN9nT59OhaLBTAW+rrlllvYt28fSikqKytr6r3zzjtrlnQOCQlBKcXNN9/MRx99xJw5c/j555/54IMPzuLVOnOS0NuSJkbSznTdddfx+eefc/z48ZpvCfr444/JzMzk119/xd3dnZiYmAaXMW2IsdzE6caOHcuaNWtYunQpN998M/PmzWPWrFls3bqV7777jtdee40lS5awaNEihx2baIIT4q28sBCPZso4It4aW2ZXN7JcbWPba2+r317t5XH/9Kc/MWHCBJYtW8bhw4cZP358k/XOmTOHyy+/HC8vL6ZPn95qa/jLHLpg5syZLF68mM8//5zrrrsOMEYkERERuLu7k5iYyJEjR1pc39ixY/n000+x2WxkZmayZs0ahg4dypEjR4iIiGD27NncdtttbNq0iaysLKqrq7n22mt56qmnapYqFebliHhrbJndoUOHNrgMbkNL5gJ06NCB3bt3U11dXTPab6y9k0vxvvfeezXbJ02axIIFC2o+OD3ZXufOnencuTNPP/20w768oiUkoQv69etHYWEhkZGRdOrUCYAbb7yRjRs3MnjwYD7++OMz+kKJq6++mri4OOLj47nooot4/vnn6dixI0lJSSQkJDB69GiWLl3K7373O9LS0hg/fjwJCQnMnj2bZ5991lmHKdoIR8RbY8vshoWFNbgMbkNL5oLxlXiXXXYZF110UU1fGvKHP/yBhx9+mFGjRtX5Muu5c+cSHR1dE++fffZZzb4bb7yRLl260Ldv37N7oc5GY8swOvvWlpcYbc32ZfnccyfL59Yly+e2jfbvvvtu/fbbb59TfWca2zKHLoQQDjZo0CB8fX156aWXWrVdSehCCOFgv/76q0valTn0NkA3claIaJ68dmdOXrPzw9n8niShu5iXlxfZ2dnyR3YWtNZkZ2fj5eXl6q6cNyTezg9nG9sy5eJiUVFRpKamkpmZ2WptlpWVuTQJOrJ9Ly8voqKiHFJXe+DseDNTbLm6/bOJbUnoLubu7l5zVVtrSUpKYsCAAa3aZltqvz1zdry5+nfb3tuXKRchhGhFh7KKKa2wNV/wLLRohK6UmgL8A7AAb2utn6u3PxD4CIi21/mi1vpdB/dVCIeSuBbOtjUlj2P5pTWPt6Xm88bqAwyMDubjucPwcrc0+DytNdvT8knPKyXE15PBXYNxc2t+YbxmE7pSygK8BlwCpAIblFLLtda7ahW7G9iltb5cKRUOJCulPtZaVzTbAyFcQOJanIlfj+QQ5ONBj/C6X7ahteazjaks3ZTKmJ5hdCqvZs/xAjILy0nPK2X+0tNXsxzTM4wf9mcx9vlEOgZ6MfqCMLak5BHi60Gorwc/HsimqKyK4wWn1paJCvbm2WtiGdMzvMl+tmSEPhTYr7U+CKCUWgxcCdQOfA34K2OVGj8gBzh/VoUX7ZHEtaCs0kbinhMcLygjwt+Li/pE4O1h4dcjOSxYfZC03FI6BXqxas8JAC6N7cTLMxNwt7ixclcG7/xwiJ8PZhMZ5M2L/91rVLp6bU39oy4I5Y/T+nJy/S5vdwsxYb58tS2d73ZmkJpbwutJB+ge7svuYwUUl9sYdUEoAd7ujOwRSmxkEPszi3h11T5mLVrPny5tehkB1dzpS0qp64ApWuu59sc3A8O01vfUKuMPLAf6AP7ADK311w3UdQdwB0B4ePigJUuWNNm2MxUVFbn0q61c2X57OPYJEyb8qrUe3Nh+R8a1vazLY7va/rdcUlzc6r/faq3ZnV3N7sxSPDxOrbcY5u3GoAgLHhbYfMKGxQ36hVpwU+BWa5XCIwU2dmTZsGnoFWyhWsP+PBtWBXHhVqL8jY/7bNWan49VERtmJdDz9CmIoqIicrUPyTk2ogPc+HJ/BfvyquuU6RbghpdVkZxjo7IabLVSoK873NLPk0Xby/GwKCJ8FIcLqpka4w7Afw5WMqKThZ7BFj7YVUGYt+LiaHcmxVjJLtWsOVJCoI8nIV6KQ/nVTOvujre16amSkkqNt9Xoh02Dp+X08uU2zSe7KxjbxcptV13caGy3ZITeUG/q/xeYDGwBLgJ6AN8rpdZqrQvqPEnrhcBCgN69e+uTS1C6QlJSEu21/fZ87LU4LK7B8bFdXa1ZsjGFg1nF7EjLZ3tqPhddGMH8KX3oHORdU668ykZ+aSXPrdjDF5vT8Ha3cGs/L+ZdVrf9Sls1JeU2vD0seFjdsFVrLC2Yk23IyboAThSW8e8taSzblEZ6fhnGy1pZp7yvh4ULOvizNeXUl5eE+Hrwzi2D2ZdRxLs/HWb3seLaLdR5/pK9lcwZFcNjl/Xl9aQDvL09mTA/Raiv8Y/jL9fEckG4H6WVNu5ZlMTGjFNz1j4eFm4YFoOH1a2m76v3ZlJQYeO6IZH4ebozpmcYfTsFsPt4AfOXbuP1LaX4eVpZcf8YooJ96iyR23PVPv72/V5+PmZjWLcQ3r91aJ158HAnxvZk48uXuK2JMi1J6KlAl1qPo4D0emXmAM/ZF47Zr5Q6hDGqWd/SzgrRytpsXGcVlfPAp1tYuy8LT6sbnQK9uLhvB/678zg70vKZM6ob2UUVhPp58Ndv9lBYXoWbgpuGR7M9NZ83t+WT7b6NuKggZg7pwi+Hcrj/081kFJTj52mlf2QAGw/n0quDP/Fdggjz82Byv454e1hI3HMCrWFCnwi83N2osmmSkk9QUmmjX+dAvtl+jK+3HaOw/NTMk5uCsb3CeXjahXhkJjNxwjjA+O+46UguSzelsu5gDg9P7UNMmC/Jxwv57NcUpi/4mapqTf/IAJ68sh+XxnbC093Cqt0ZWNwUF/WJoKTCxssr9/Luj4dJyy0lMfkEY3qGUVRehZtSHMsr5do3fqrpi0XB/7ukF1NjO7LpaB5DYkLoFuZb/yVu0MgeYXxw6zDu/PBXfju+B1HBPkDdNdPvm9iTy+I68cP+LK4aENnoh5qu0pKEvgHoqZTqBqQBM4Eb6pU5CkwE1iqlOgC9gYOO7KgQDtYm4/rnA9n8bvFm8ksr+cvVsVw/tEtNQvnlYDY3L1rPo//eUVN+UNdgLo3txMCuwSR0CSK3uIKZ/1zFdzuPs3hDCq8n7Sc1t5Tu4b48eml39hwvZEtKHjOGdGFHegHf78ogt6SCV/+3v04/nlmxu8H+ebtbmBrbkX6dA1GAt4eFiX0iiAgwLqZJStqL1XLqbOhh3UMZ1j20Th2T+3Xk8vjO/G7xZi6P68zcMd3qJM0rEyJr7vt4WHnyiv64KcWK7cfp2zmQl2ckEOrnCUBucQVfbT9GZZUxrWLJOcgtE3sCcEGE/xm99gDdwnz57oGxTZbpHu5H93DXTVk2pdmErrWuUkrdA3yHcXrXIq31TqXUnfb9C4CngPeUUtsx3nPN11pnObHfQpyTthTXtmrN89/uYdexAn7cn0VMmC/v3zqUCzsF1Ck3rHsoX/x2JLZqTacgL3YfK2Rkj1DcayXQYF8PHh7mzbhx4/hw3RG+3naMG4d1ZdaIrvh6NvznnlVUzg/7sqi0VTOwazBuSrHxcE7N/FNcVCB+ntaa9hqr50x0C/Nl+T2jW1TWzU3x5JX9efLK/qftC/b14ObhXWseJyW1/ItYzKhFvxmt9QpgRb1tC2rdTwfkyyDFeaWtxPW7Px7izTUH6dPRn+uHRvPItAsbTZr9IwNr7kf4N36JuVKKWSNimDUiptn2w/w8uWpAZJ1tDU1TnJyCEG2XXPovhIt8uSWNhWsOsi+jiIsvjOCtWYOb/JZ7IZojCV0IF/jfngweXLKVC8L9uCKhM/On9JFkfj6yVYGtAjzO8d1LRTFYvcDN/iFreSF4+kN1NeQegpOnl/uENFmNJHQhWll5lY35S7fTu4M/n905wiFz0sIFKorhg6sgPxVu+w6Cos+unqz98O4UiLgQbvwcDq2FxdfDwFsg7yjs++5U2YsebbIqiSQhWtnyLelkFpbz0vR4Sebnk3ULoLIERj8A1Tb4bA6kbQR3X3hnMoT1JD43F44En1m9mXugqgIOrYGF4yH3MLh7w4a3jP1j/wBhvYz7HfoBf2i0KokmIVpRdbXmnR8O0buDP2N6hrm6O85XVQFZydAxtta2cjiQCLbyU9tCe0KHvpD2qzHiPcnDD7qPN6YiKkuN51XXvfAIAIsHdJ+AtbIIMnYZdYFRV9oZfB2cdwjEjIa8I3Bs66ntx7fDmheM+yXZUHTCGDlf+jfo0B8Sn4aqctyqK43jOxMRF8LFTxhtbP4YelwE016E9W+Cf2cYdkeLq5KELkQr+uiXI+w5XsjfZ8S3jznzVX+Gn/8JV74GA24y5pw/mw3JK+qWc7PC4NuMJFbfoDkw9a/wrxlwaHXjbXWfQELGQfgpFa7/BEK6wzuToDTnzPo88BbYuQzK610Q3OcycPcxjgdg3HwYYr9u85b/ALD5XK4U7TwABs469fjiJ864CknoQrSS1NwSnl2xh7G9wrkqIbL5J5wvqipg2f+BVyD4XXFqe2ke/PqeMXpefh/4hsOer41kPulpYyQKUF0F/77bSObdxsHkv1CzmtWWfxkJdMdSI8FOexG6jjy9D4fWwrfz8VFWCO0Bi28AN3fw9IPZK8A7qGXH8tOrsOl9CIiEm5aCh/30TWUxpj2UgrHzwGI1/mG0MZLQhXAWWxVs/wz6TAOvQJZtSqO00sYzV/U3x+i8LB/WvwVH18H+7wHoG54M/GLsP7ELKopgzjfw7UPwyUzQ1TDm9zDy3rp13bQUNn8IQ+8Ar1oXVE162kiu2fug6yiIva7hvnToBz6hbN+fSvykm+CnfxhTH4PmnJp+aYkrXjWmQHpfCmEXNFwmvFfL62tlktCFcIaiE7DqSSNJDb8LpjzLqj0niO8SRJeQszzFraLYuDXDvSLPaN87xBhJ2irB4n52bdoqjemQhv4B/fgKrH3R2H/Jk1CaS/gPL8PqU2ur0OcyY0R94+fw4TUQPazhMzX8O8DY35++XSkYcVfL+ho3ndycJPALN/4RnA2LO4z63dk9tw2QhC6Eg3mVZcCLxnoi+HWETR+QNfhBtqbm8cDFZzm6O7QW/vUb4yyLZowC+AkI7wOjH4Sv7ofxD8Oo+86szewD8O40iBkF17x16hxpMP6xbHzHSNgzP67ZvNo6vuE5ZL8I+O0PZ9a+OGOS0IVwMPfKIrjweoj7DQRGGaeifXwt97r15KLeLzb95LRf4ad/nn4mx8HVxnnOQ29vtv29e/fRKyYSEp+BZXeA1Ru+/xMc+fHMRuppm4xplR1LIT/NGPmeVJwFpbkw4p7Gny9anSR0IZxh6l8hoDMAevBtlG3+jgfdt6K3BEH5ZUaZDv3BNwxS1hsj74oi+PIeY5rBv1Pd+jonwFVvGP8gmpFenESvUeMhvDdsfBemvWAk99qn4bVEQGf4zYdwMPHUh5K1xd8A0cPPrE7hVJLQhXCwYjc/bv4shUWzO+JuceO7mHnc+cNF/LfP1/Ta+DZsfNsoGDnImK9dUutUNd8ImPs9BMece0d6TTZuAFcvaLpsU6IGNTy/LdocSehCOFiqLYjSfVm8kXSAu8b34IXvkrkgwp/uN/0TTvyfcYHMwURY/VdY8Qfj9LcrXzeeHN672fU6hGiMJHQhHKwCd0ZdEMorq/aRllvKgcxi3rx5EFarxZg6AegUb5zyV3Tcfm71CNd2WpiCW/NFhBBnwseqeP3GQXQP9+XTjSkMjA5iUt8OdQt5+BhnnQREQUL9L0oS4uxIQhfCwYI8FYHe7rw3ZyiX9O3Ak1c2ciHRqPvhgR2nrkYU4hzJlIsQDuZhP127c5A3b80a3HhBM1wtKtoUGaELIYRJSEIXQgiTkIQuhBAmIQldCCFMQhK6EEKYhCR0IYQwCUnoQghhEpLQhRDCJCShCyGESUhCF0IIk3BZQs8u065qWgghTMllCb2wQnM0u/nvRxRCCNEyLp1yWbY5zZXNCyGEqbgsoXtZYNnmVLSWqRchhHAElyX0CLdCDmeXsO5gjqu6IIQQpuKyhB5YlUVv32LeWH3AVV0QQghTaVFCV0pNUUolK6X2K6UeaqTMeKXUFqXUTqXU6uZr1TwRvZU1ezPZlpp3Zr0WwgGcE9dCuE6zCV0pZQFeA6YCfYHrlVJ965UJAl4HrtBa9wOmN1evzeLNsNyvCPRy45VV+8+q80KcLWfFtRCu1JIR+lBgv9b6oNa6AlgMXFmvzA3AF1rrowBa6xPNVVrpEYBb3mGe7J/Jyt0ZMkoXrc0pcS2EK7XkO0UjgZRaj1OBYfXK9ALclVJJgD/wD631B/UrUkrdAdwBEBEeToV7ECPS3sfX/UEeXbyOBwd7nc0xnJWioiKSkpJarb221H57PvZaHBbXUDe2w8PD2+3r6+rfbXtvH611kzeMt5lv13p8M/BqvTL/BNYBvkAYsA/o1VS9vXr10vp/f9H68QD98df/013nf6V/PZKjW0tiYmKrtdXW2m8Pxw5s1C6Ia30ytl1IYsvc7TcV2y2ZckkFutR6HAWkN1DmW611sdY6C1gDxDdb8+Bbwc2d6VXLCfX14O/f721Bd4RwCOfFtRAu0pKEvgHoqZTqppTyAGYCy+uV+RIYo5SyKqV8MN667m62Zv8OED8D923/4v4Rwazdl8UP+7LO8BCEOCvOi2shXKTZhK61rgLuAb7DCOYlWuudSqk7lVJ32svsBr4FtgHrMd7K7mhRD0b+DqrKuV6voEuIN09/vQtbtVw9KpzL6XEthAu05ENRtNYrgBX1ti2o9/gF4IUz7kF4L+h7Bdb1C3h80lXMXZrCko0pXD80+oyrEuJMODWuhXCBtrEe+sTHwVbOxGPvMKhrMC/9dy/F5VWu7pUQQpxX2kZCD+0BQ25Hbf6AZ4aWk1VUzgJZEkCcicpSkIXeRDvXoimXVjHhYdj1JX1+eYSr4/7Bm2sOMn1QF6JDfVzdM9GWVJXD/lVQmgNuVshLgX3/hdT1EBAFvafAgJtc3UshXKLtJHSvQLjs7/DJDJ7ssZLv3Aby1Ne7eGvWYFf3TLhCcRbkHgbfcMjeBzu+gMJjkLUP8lPqlu0UD6MfgOwDsPkj2LOiwSqFMLu2k9DBGF31uxr/9X/nj8P+xR/XZvDTgSxG9ghzdc+Eo5XmQt5RKMkGZQEPXyjLg0NrIHMvHPgf2MpPlfcMgLCeENIdLn0JwnuDrgavIPAJqSxY2DQAABVsSURBVFVvnpHYfy8DAdH+tK2EDjD1eTiQyPXpz/Fm4EM8/dVu/nPvaCxuytU9Ey2VewRO7DJ+ulkgoi9EjzC2JT3L6H2JkFTc8HMtHkbSHngzdJ8AJVkQFA1dhoNHC6bfvIMgapBjj0eI80TbS+h+EXDpS7gtvY2FF65myuYRLN5wlBuHdXV1z9qvzGSorjISc85BYw770Grw6wCTnoKiDMjYBce2QPK3kLH99Do8A6E8HzwDOBExms6xYyCoK/iGGSPtylKwuEPU0JYlbiHEadpeQgeIvQ6Sv6H3rgVc16Ufz3+bzJR+HQn183R1z85vtiqorsRaWQD7VkJWMqSsB1sl9LwYfEKhQ3/I2AFHfoKyfDix20jUAFZvqCo17gdGQ0EqbF0MlfbRtrJA5wEw+VmIGmyMtKttRvLfvwoiB0Hsdexdv43OI8e75CUQwszaZkIHmPo86mAiT6sFfFXxe578ahf/mDnA1b1q2wqPw5EfjVFuUBcjWe/60vggMXM35KeBtjEa4Ef7cwKiQClI/rpuXe6+4B0MwTFwyZPg6Q8n9kBEH+g2zjjVdP8q2PYpRA2ByIEQfmHDo+u43xg3IYRTtd2E7hsK017A6/Nb+aTr11y95XKmxXZicr+Oru6ZaxVnQXmhcaZHynooOmEk8ZIcKD5hTI0AePhDRSFYPCH0AiPpxv4GPHw5cOgwPcZMh4gLjVE5GGeUlOVD+ibw6wi9Jhvz3025YKJxE0K0CW03oQP0vxZSNjDglzd4NNibh5a6ExcVSKdAb1f3zHlKcoyzPyzukLzCuG+rNKYucg5A2iag1gU07r7G9EbnBPAJg15TjKSce9j+4eIs4wySWlJsSfToNqZuuyHdjJ+dE5x6eEII52nbCR2MD90KjzF31yKUzuKej3355I4ReFjbxkWu56Q4Cw7/AAdWQcoG47S9wmO1Cijjg0eLByiM6ZEJj0BglHG6XrcxxlRIfV1HtNYRCCHakLaf0C3ucN0iWBHKbRvfIfBYHs/9+488dt1wV/esZUpy4KdXIXWDMaXh1wE6xjJk0xJISjXKeAUap+X5DYKgGGPBsvIi6DEBAjq7tPtCiPNH20/oYMzlXvoS+IZx7ernGbN9Bj9U3sXo6Q+A1cO1fasoMea0K4qMM0KO/mycgudmMea3DyYa+yMHg38nYwpl//fY/HvCpKeN7VFDwHJ+/CqEEG3X+ZNFlIIJj1B9wWSKPr6X0cl/ofjFRfiOuQsuvOLUHLAzVVUYVzge3wb7vjfmqtM3n/ogEsDqZcxZa21Mh/SeBiPugY79T5UpL2LTTxsYP3KC8/sshGg3zp+EbmfpMoio/7eGFxa+yfiM9xjy/WPw/WMQ2tP4QC/iQugYB2G9QNuMi1eaOluj2gYoyDtsXDZelg+Ze8BWYSwElXfEuJS8NBey959K3u6+xhoiI+815rStXqf6YG3mfHlPP+MflBBCONB5l9ABPN2t3Pd/d3LXR0N5MHk7L8cfY5BtKxz5GbZ/Vq9wAPh3NH4GdDYSb+ExhmYcgJ+LjasXm+ITZqwbEnoB9J4KAZHGudndxjafuIUQohWdlwkdwNNq4fWbBnLnh5prt3Tg7gnX8v+u741bRQEc22Zcoq4UpG8xllotzTUuYa8qBb+OFPl1w6dbP+M8bF1tjLJ9w8HdGyL6GT+tXjK3LYQ4b5zX2crTauHNmwfz2Jc7eC3xAHuOFfK3GQkEdhtjnNIHxnnYDdiVlETE+PGt11khhHCy8/5kbg+rG89eE8uTV/Zj9d5Mpry8hsTkE67ulhBCtLrzPqEDKKWYNSKGz387El9PK3Pe3cDc9zew53iBq7smhBCtxhQJ/aSELkF8fd9o5k/pwy8Hc5j6j7XM/3wbGQVlru6aEEI43Xk9h94QT6uF347vwfVDu/Ba4n7e/fEwn29KZWKfCG4f253BXYNRcsqgEMKETJfQTwry8eCPl/blpuFdWbwhhU/WH+W/uzKICfXh8vjOdKqodnUXhRDCoUyb0E/qGurL/Cl9uPeiC/jP1nS+3JLOa4n7qdbw0YG1jOsdzsDoYAZ1DSbE18XLCAghxDkwfUI/ycfDyowh0cwYEs2JwjL+vnQtB8qsvLXmIFXVxnK0/SMDGN8rgt4d/RkSE0LHQC8X91oIIVqu3ST02iL8vZgc48748SMorbCxIz2fXw5mk5icyetJxugdoEuIN3FRQfTtFEBClyD6dPSXr8ETQrRZ7TKh1+btYWFITAhDYkK456KelFXa2H+iiHUHs9l4OJftqfl8ve3UGuUBXla6hfsRGxlAv86BxIT60quDnyR6IYTLtfuEXp+Xu4X+kYH0jwxkrv1i0/zSSral5pF8vJAj2SXsO1HIl5vT+Wjd0ZrnRQV70yXYh5gwH3p38KdXR3+6BPvQMdALd4upzg4VQrRRktBbINDbnTE9wxnTM7xmW3W1Ji2vlMPZxexML2D3sQKO5pTw7Y7jfLI+paacxU0RE+pDVLAP4f6edA7yJibUh5xcGxcWlBHu54mbm5xGKYQ4d5LQz5Kbm6JLiA9dQnzqJHqtNccLyjhwopjU3BJSckvYl1HE8YIyko8XklFYhrbP0T/9yyo8rW7EhPrSMdCLCH9P/Lys+Hu50yPclx7hfnQM9CLYxwOLJH0hRDMkoTuYUopOgd6NfpF1eZWNlJxSvkpaR2h0T45mF3Moq4SMgjL2HC+gpNxGcUVVzQezAB4WN7qEeNM11JcOAZ6E+XnSMdCLriHGPwJPqxvh/p54uTex7rsQwvQkobcyT6uFCyL8SIiwMn541wbLlFfZOJxVwsHMIk4UlpOeX8rhrGKO5pSyLTWfnOLyOgkfwOqmiAz2xtfDSo8IP7oEexMT5kvfTgHEhPni62GRK2SFMDlJ6G2Qp9VC747+9O7o3+B+W7UxrXM02xjZV1RVcySnmJScUgrKKtl8NJdvth+rOb/+pMggb3p28MNSWs5B6yGigr0J9HYn1M+DbmF+Mq0jxHlOEvp5yOKmiAzyJjKo4WkdMJL+kexi9hwv5GhOCSXlVRzKNkb9B09Userorjrl3S2KQG93gnw8CPPzIDrEh9ioILqG+BDq54Gvh5VAb3eC5WpaIdqsFiV0pdQU4B+ABXhba/1cI+WGAOuAGVrrzx3WS3HGLG6K7uF+dA/3O21fYmIicUNGkppbSlF5Fcfzy9h3ooj80krySyvILCxn5e4TLNmYetpzOwZ40cE+bx/q60G/zgF42D/YvSDCj85B3ufNXL7EtTCbZhO6UsoCvAZcAqQCG5RSy7XWuxoo91fgO2d0VDiOUopQP88mL4bSWpOeX0Zabik5xRWUVFSRVVTOnmOFZBaVU1FVzfa0fL7Zcfy0555M9h5WNzoEeNGrg7/9vie9OviTWVJNWaXNpYlf4lqYUUtG6EOB/VrrgwBKqcXAlcCueuXuBZYCQxzaQ+ESSjU/rQNQWmGjsrqafRlFHM4q5nhBGXklFWQXV1Bp0xzNLmb51nQqbdWUVNhqnjdvzbcEeFmJDPYh1NeDYF8POgV6EeTjTscALyKDvOkQ4EWwrwcBXlZnfKArcS1MpyUJPRJIqfU4FRhWu4BSKhK4GriIJgJfKXUHcAdAeHg4SUlJZ9hdxykqKmq37Tur7VAgVAG+9htAJwBj3r24UnO0oJqU3FLKlQe55Zrs0mKOZxWxN12TU6apamBVY4uCMG+Fn7vCwwJh3m74eSh83cHbqvBwgxAvN3zcwddd4e/RouTvsLi2l5XYdnHb0n7LEnpDfx31TprjZWC+1trW1EhKa70QWAjQu3dvPd6FX9KclJREe22/LR97SUUVx/LLSM8r5URBObn20f7R7BIKyiopKq9iT24pBaWVlDeU/aHZdxV2DotrkNhuC21L+y1L6KlAl1qPo4D0emUGA4vtQR8GTFNKVWmt/+2QXop2w8fDSo9wP3o08GFufaUVNorKqyitsJGeX0pRWRV5pZUA/PRws0+XuBam05KEvgHoqZTqBqQBM4EbahfQWnc7eV8p9R7wlQS9cDZvDwveHsYHq9GhPmf6dIlrYTrNJnStdZVS6h6MT/ktwCKt9U6l1J32/Quc3EchHE7iWphRi85D11qvAFbU29ZgwGutZ597t4RwPolrYTayULcQQpiEJHQhhDAJSehCCGESktCFEMIkJKELIYRJSEIXQgiTkIQuhBAmIQldCCFMQhK6EEKYhCR0IYQwCUnoQghhEpLQhRDCJCShCyGESUhCF0IIk5CELoQQJiEJXQghTEISuhBCmIQkdCGEMAlJ6EIIYRKS0IUQwiQkoQshhElIQhdCCJOQhC6EECYhCV0IIUxCEroQQpiEJHQhhDAJSehCCGESktCFEMIkJKELIYRJSEIXQgiTkIQuhBAmIQldCCFMQhK6EEKYhCR0IYQwiRYldKXUFKVUslJqv1LqoQb236iU2ma//aSUind8V4VwLIlrYTbNJnSllAV4DZgK9AWuV0r1rVfsEDBOax0HPAUsdHRHhXAkiWthRi0ZoQ8F9mutD2qtK4DFwJW1C2itf9Ja59ofrgOiHNtNIRxO4lqYjrUFZSKBlFqPU4FhTZS/DfimoR1KqTuAOwDCw8NJSkpqWS+doKioqN22356PvRaHxTVIbLeFtqV9QGvd5A2YDrxd6/HNwKuNlJ0A7AZCm6u3V69e2pUSExPbbfvt4diBjdoFca3beWy3h9hydftNxXZLRuipQJdaj6OA9PqFlFJxwNvAVK119pn+YxGilUlcC9NpyRz6BqCnUqqbUsoDmAksr11AKRUNfAHcrLXe6/huCuFwEtfCdJodoWutq5RS9wDfARZgkdZ6p1LqTvv+BcBjQCjwulIKoEprPdh53Rbi3EhcCzNqyZQLWusVwIp62xbUuj8XmOvYrgnhXBLXwmzkSlEhhDAJSehCCGESktCFEMIkJKELIYRJSEIXQgiTkIQuhBAmIQldCCFMQhK6EEKYhCR0IYQwCUnoQghhEpLQhRDCJCShCyGESUhCF0IIk5CELoQQJiEJXQghTEISuhBCmIQkdCGEMAlJ6EIIYRKS0IUQwiQkoQshhElIQhdCCJOQhC6EECYhCV0IIUxCEroQQpiEJHQhhDAJSehCCGESktCFEMIkJKELIYRJSEIXQgiTkIQuhBAmIQldCCFMQhK6EEKYhCR0IYQwCUnoQghhEpLQhRDCJFqU0JVSU5RSyUqp/UqphxrYr5RSr9j3b1NKDXR8V4VwLIlrYTbNJnSllAV4DZgK9AWuV0r1rVdsKtDTfrsDeMPB/RTCoSSuhRm1ZIQ+FNivtT6ota4AFgNX1itzJfCBNqwDgpRSnRzcVyEcSeJamI61BWUigZRaj1OBYS0oEwkcq11IKXUHxkgHoFwpteOMeutYYUBWO22/PRx712b2OyyuQWK7jbTdXtpvNLZbktBVA9v0WZRBa70QWAiglNqotR7cgvadoj23356PvXY3Gth2VnENEtttoW1pv2VTLqlAl1qPo4D0sygjRFsicS1MpyUJfQPQUynVTSnlAcwEltcrsxyYZT8rYDiQr7U+7W2pEG2IxLUwnWanXLTWVUqpe4DvAAuwSGu9Uyl1p33/AmAFMA3YD5QAc1rQ9sKz7rVjtOf22/OxA06Na3D98UlstdP2ldYNTgkKIYQ4z8iVokIIYRKS0IUQwiRcktCbu+TawW11UUolKqV2K6V2KqV+Z9/+hFIqTSm1xX6b5sQ+HFZKbbe3s9G+LUQp9b1Sap/9Z7CT2u5d6xi3KKUKlFL3O/P4lVKLlFInap+L3dTxKqUetsdCslJqsqP60dpaM67t7bXb2Ja4boTWulVvGB9AHQC6Ax7AVqCvE9vrBAy03/cH9mJc6v0E8PtWOubDQFi9bc8DD9nvPwT8tZVe++MYFyY47fiBscBAYEdzx2v/XWwFPIFu9tiwtGZMOvC1bbW4trcpsa0lrmvfXDFCb8kl1w6jtT6mtd5kv18I7Ma42s/VrgTet99/H7iqFdqcCBzQWh9xZiNa6zVATr3NjR3vlcBirXW51voQxhklQ53ZPydp1bgGie1aJK7tXJHQG7uc2umUUjHAAOAX+6Z7lLGK3iJnTXnYaeC/SqlflXGJOEAHbT+n2f4zwontnzQT+KTW49Y6fmj8eF0WDw7m0uNo57EtcW3nioTe4supHdqoUn7AUuB+rXUBxsp5PYAEjLU5XnJi86O01gMxVu+7Wyk11oltNch+8cwVwGf2Ta15/E12rYFt5+O5tC47jvYc2xLXdbkiobf65dRKKXeMgP9Ya/0FgNY6Q2tt01pXA2/hxLdDWut0+88TwDJ7WxnKvnKf/ecJZ7VvNxXYpLXOsPel1Y7frrHjNcvl9S45DoltievaXJHQW3LJtcMopRTwDrBba/23WttrL4N6NeCU1fGUUr5KKf+T94FJ9raWA7fYi90CfOmM9mu5nlpvS1vr+Gtp7HiXAzOVUp5KqW4Ya4+vd3JfnKFV4xoktu0krmtz9qeujXxaPA3jE/kDwB+d3NZojLc624At9ts04ENgu337cqCTk9rvjvFp91Zg58njBUKBVcA++88QJ74GPkA2EFhrm9OOH+MP7BhQiTFSua2p4wX+aI+FZGCqK2LSQcfdanFtb69dx7bE9ek3ufRfCCFMQq4UFUIIk5CELoQQJiEJXQghTEISuhBCmIQkdCGEMAlJ6CailBqvlPrK1f0QwpEkrltOEroQQpiEJHQXUErdpJRab1+v+U2llEUpVaSUekkptUkptUopFW4vm6CUWmdfbGjZycWGlFIXKKVWKqW22p/Tw169n1Lqc6XUHqXUx/arCYVwOolr15OE3sqUUhcCMzAWNUoAbMCNgC/GmhQDgdXA4/anfADM11rHYVwBd3L7x8BrWut4YCTGFWxgrLh3P8Z6zN2BUU4/KNHuSVy3DVZXd6AdmggMAjbYBxneGAv6VAOf2st8BHyhlAoEgrTWq+3b3wc+s6+fEam1XgagtS4DsNe3Xmudan+8BYgBfnD+YYl2TuK6DZCE3voU8L7W+uE6G5X6U71yTa3J0NTbzfJa923I71i0DonrNkCmXFrfKuA6pVQE1HwnYVeM38V19jI3AD9orfOBXKXUGPv2m4HV2ljzOlUpdZW9Dk+llE+rHoUQdUlctwHyX66Vaa13KaUexfiWFzeMldvuBoqBfkqpX4F8jPlIMJbkXGAP7IPAHPv2m4E3lVJP2uuY3oqHIUQdEtdtg6y22EYopYq01n6u7ocQjiRx3bpkykUIIUxCRuhCCGESMkIXQgiTkIQuhBAmIQldCCFMQhK6EEKYhCR0IYQwif8PhwtIaVjs6dkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlim([0,120])\n",
    "plt.ylim([0,1.0])\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlim([0,120])\n",
    "plt.ylim([0,1.0])\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that both the training loss and validation loss decrease rapidly before 25 epochs. After that the gap between training loss and validation loss get larger, which means the model starts to overfit. In our later lectures, we will talk about what overfitting is, what causes overfitting and how to prevent overfitting when training deep learning models.\n",
    "\n",
    "In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training. But that's not the case. Indeed, the validation loss is computed at the *end* of each epoch, while the training loss is computed *during* each epoch. So the training curve should be shifted by half an epoch to the left. If you do that, you will see the training and validation curves overlap almost perfectly at the beginning of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the model\n",
    "Once you are satisfied with your model's validation accuracy, you may evaluate the performance on the test set to estimate how the model generalize to new data. You can easily do this using the `evaluate()` method. The first output of `evaluate()` method is the loss of the model, and the second is the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 749us/step - loss: 0.4625 - accuracy: 0.7857\n",
      "Accuracy on the test set: 78.57\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on both the test set\n",
    "_,accuracy = model.evaluate(X_test,y_test)\n",
    "print('Accuracy on the test set: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also use the `predict()` method to make predictions on new instances. The `predict()` outputs the likelihood of the diabetes. You need get the output class (0 or 1) by thresholding the probability with 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8354354969841159, 2.4573590260372535, 0.3467431597703781, 1.3950740095371392, -0.6996567372359558, 1.3522451320376832, 2.7859441674361065, -0.9656918919706856] => 1 (expected 1)\n",
      "[-0.5448080776366672, -0.4371963301807875, 0.24436263555830207, 0.5845724632728228, 0.15216202200991366, 0.17619533491603662, -0.18763809923801236, -0.8824028314748293] => 0 (expected 0)\n",
      "[0.036446761058230505, -1.4125356349933882, -0.3699205097141541, -1.2858157204140617, -0.6996567372359558, 0.2267781218890105, -0.2266851391034301, -0.7158247104831167] => 0 (expected 0)\n",
      "[0.3270741804056793, 1.41909460478513, 0.14198211134622604, -0.10123653741236822, 0.7910260914443157, -0.8101690110569577, 0.36202407732902164, 1.4496908624091467] => 1 (expected 1)\n",
      "[-1.1260629163315647, -0.31134609730174223, -0.21634972339604003, -1.2858157204140617, -0.6996567372359558, -0.9619173719758798, 0.5812882242655983, -0.21609034750797898] => 0 (expected 0)\n",
      "[-0.8354354969841159, -1.2552228438945816, 0.14198211134622604, -0.16358281020193105, -0.358929233537608, -0.7090034371110094, -0.5510759318315159, -0.799113770978973] => 0 (expected 0)\n",
      "[1.4895838577954748, 0.9786187897084716, 0.4491236839824541, 0.833957554431074, 0.756953341074481, 0.2520695153754979, 1.2811313233919313, -0.049512226516266404] => 1 (expected 1)\n",
      "[-0.2541806582892183, 1.7337201869827432, 0.8586457808307582, 0.3975336449041343, 0.6291805271876005, 0.13825824468630554, 2.0650757391514714, 1.5329799229050032] => 1 (expected 1)\n",
      "[-0.5448080776366672, 0.19205483421443878, -0.5746815581383061, 0.21049482653544582, 1.6428448506901852, -0.569900772935331, 3.404689568380419, -0.7158247104831167] => 1 (expected 0)\n",
      "[0.6177015997531281, -0.5630465630598328, 0.14198211134622604, 0.7092650088519483, 0.9187989053311961, 0.6946689013890209, -0.4279275753328907, 1.782847104392572] => 1 (expected 0)\n"
     ]
    }
   ],
   "source": [
    "y_predict = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "for i in range(10):\n",
    "    print('%s => %d (expected %d)' % ((X_test[i]).tolist(), y_predict[i], y_test[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
